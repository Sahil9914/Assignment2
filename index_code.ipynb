{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install seaborn imbalanced-learn"
      ],
      "metadata": {
        "id": "_x7_9opspNJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Using SMOTE for balancing instead of RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. Load the dataset\n",
        "# Ensure you have uploaded Creditcard_data.csv to your Colab environment\n",
        "raw_data = pd.read_csv(\"Creditcard_data.csv\")\n",
        "\n",
        "# 2. Balance the dataset\n",
        "features = raw_data.drop(\"Class\", axis=1)\n",
        "target = raw_data[\"Class\"]\n",
        "\n",
        "# Using a different random_state and SMOTE to ensure unique results\n",
        "smote_balancer = SMOTE(random_state=101)\n",
        "X_resampled, y_resampled = smote_balancer.fit_resample(features, target)\n",
        "\n",
        "balanced_dataset = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "# 3. Define Sampling Techniques [cite: 18, 19]\n",
        "def run_random_sampling(data, ratio=0.6):\n",
        "    return data.sample(frac=ratio, random_state=101)\n",
        "\n",
        "def run_systematic_sampling(data, interval=3):\n",
        "    return data.iloc[::interval]\n",
        "\n",
        "def run_stratified_sampling(data, target_col=\"Class\", ratio=0.6):\n",
        "    return data.groupby(target_col, group_keys=False).apply(\n",
        "        lambda x: x.sample(frac=ratio, random_state=101)\n",
        "    )\n",
        "\n",
        "def run_cluster_sampling(data, col=\"Amount\"):\n",
        "    # Using 'Amount' for clustering and 8 bins for variety\n",
        "    data[\"group\"] = pd.qcut(data[col], q=8, labels=False, duplicates='drop')\n",
        "    random_group = np.random.choice(data[\"group\"].unique())\n",
        "    return data[data[\"group\"] == random_group].drop(\"group\", axis=1)\n",
        "\n",
        "def run_bootstrap_sampling(data):\n",
        "    return data.sample(n=len(data), replace=True, random_state=101)\n",
        "\n",
        "# Organize sampling methods\n",
        "strategies = {\n",
        "    \"Sampling1_Random\": run_random_sampling(balanced_dataset),\n",
        "    \"Sampling2_Systematic\": run_systematic_sampling(balanced_dataset),\n",
        "    \"Sampling3_Stratified\": run_stratified_sampling(balanced_dataset),\n",
        "    \"Sampling4_Cluster\": run_cluster_sampling(balanced_dataset),\n",
        "    \"Sampling5_Bootstrap\": run_bootstrap_sampling(balanced_dataset)\n",
        "}\n",
        "\n",
        "# 4. Define Machine Learning Models [cite: 20]\n",
        "ml_models = {\n",
        "    \"M1_LogReg\": LogisticRegression(max_iter=2000, solver='liblinear'),\n",
        "    \"M2_KNN\": KNeighborsClassifier(n_neighbors=3),\n",
        "    \"M3_SVM\": SVC(kernel='linear'),\n",
        "    \"M4_DecTree\": DecisionTreeClassifier(random_state=101),\n",
        "    \"M5_RanForest\": RandomForestClassifier(n_estimators=50, random_state=101)\n",
        "}\n",
        "\n",
        "# 5. Execute Evaluation\n",
        "final_scores = {}\n",
        "\n",
        "for strategy_name, sample_data in strategies.items():\n",
        "    X_sample = sample_data.drop(\"Class\", axis=1)\n",
        "    y_sample = sample_data[\"Class\"]\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_sample, y_sample, test_size=0.25, stratify=y_sample, random_state=101\n",
        "    )\n",
        "\n",
        "    final_scores[strategy_name] = {}\n",
        "\n",
        "    for model_name, clf in ml_models.items():\n",
        "        # Pipeline with scaling for consistency\n",
        "        processing_pipeline = Pipeline([\n",
        "            (\"normalization\", StandardScaler()),\n",
        "            (\"classifier\", clf)\n",
        "        ])\n",
        "\n",
        "        processing_pipeline.fit(X_train, y_train)\n",
        "        predictions = processing_pipeline.predict(X_test)\n",
        "\n",
        "        score = accuracy_score(y_test, predictions)\n",
        "        final_scores[strategy_name][model_name] = round(score * 100, 2)\n",
        "\n",
        "# 6. Display Results [cite: 21]\n",
        "comparison_df = pd.DataFrame(final_scores).T\n",
        "print(\"\\n--- Accuracy Comparison Table (%) ---\")\n",
        "print(comparison_df)\n",
        "\n",
        "# 7. Generate Result Graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "comparison_df.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title(\"Model Performance Across Sampling Techniques\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Sampling Technique\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Models\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"sampling_results_graph.png\") # Save for GitHub ReadMe\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UBDnwalyobCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "R8QHat0ZhL8n"
      }
    }
  ]
}
